{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we now explore how to use litellm with `Gemini API` to power autogen. This work is draws heavily on [autogen examples](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_custom_model.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=center> LiteLLM </h2>\n",
    "\n",
    "We will need the `litellm` python client. This makes a call to gemini and returns a familiar OpenAI-like response that autogen can actually use. This greatly eases our design. It would be possible to complete this task without it but would require us to define this interface ourselves.\n",
    "\n",
    "### Prerequisites\n",
    "We will follow the litellm docs on [gemini](https://docs.litellm.ai/docs/providers/gemini).\n",
    "\n",
    "Ensure you have the python client installed\n",
    "\n",
    "```bash\n",
    "pip install litellm\n",
    "```\n",
    "\n",
    "You may require a `GEMINI_API_KEY` environment variable set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=center> Custom Autogen Model Class </h2>\n",
    "\n",
    "This is an attempt to leverage LLM to ease the process of creating a custom model class for `autogen`. It needs to implement a particular protocol, the `ModelClientResponseProtocol.Choice.Message` protocol, to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol, Optional, List, Union, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROTOCOL CLASS\n",
    "# Defines an OpenAI response interface\n",
    "class ModelClientResponseProtocol(Protocol):\n",
    "    class Choice(Protocol):\n",
    "        class Message(Protocol):\n",
    "            content: Optional[str]\n",
    "\n",
    "        message: Message\n",
    "\n",
    "    choices: List[Choice]\n",
    "    model: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL CLASS\n",
    "class CustomModelClient:\n",
    "    \"\"\"\n",
    "    A client class must implement the following methods:\n",
    "    - create must return a response object that implements the ModelClientResponseProtocol\n",
    "    - cost must return the cost of the response\n",
    "    - get_usage must return a dict with the following keys:\n",
    "        - prompt_tokens\n",
    "        - completion_tokens\n",
    "        - total_tokens\n",
    "        - cost\n",
    "        - model\n",
    "\n",
    "    This class is used to create a client that can be used by OpenAIWrapper.\n",
    "    The response returned from create must adhere to the ModelClientResponseProtocol but can be extended however needed.\n",
    "    The message_retrieval method must be implemented to return a list of str or a list of messages from the response.\n",
    "    \"\"\"\n",
    "    def __init__(self, config) -> None:\n",
    "        self.model = config.get(\"model\", \"gemini/gemini-pro\") \n",
    "\n",
    "    def create(self, params) -> ModelClientResponseProtocol:\n",
    "        ## Get messages from agents\n",
    "                \n",
    "        response = completion(\n",
    "            model=self.model, \n",
    "            messages=params[\"messages\"]\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "\n",
    "    def message_retrieval(\n",
    "        self, response: ModelClientResponseProtocol\n",
    "    ) -> Union[List[str], List[ModelClientResponseProtocol.Choice.Message]]:\n",
    "        \"\"\"\n",
    "        Retrieve and return a list of strings or a list of Choice.Message from the response.\n",
    "\n",
    "        NOTE: if a list of Choice.Message is returned, it currently needs to contain the fields of OpenAI's ChatCompletion Message object,\n",
    "        since that is expected for function or tool calling in the rest of the codebase at the moment, unless a custom agent is being used.\n",
    "        \"\"\"\n",
    "        choices = response.choices\n",
    "        return [choice.message.content for choice in choices]\n",
    "\n",
    "    def cost(self, response: ModelClientResponseProtocol) -> float:\n",
    "        return 0\n",
    "\n",
    "    @staticmethod\n",
    "    def get_usage(response: ModelClientResponseProtocol) -> Dict:\n",
    "        \"\"\"Return usage summary of the response using RESPONSE_USAGE_KEYS.\"\"\"\n",
    "        return dict(response.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list_custom = [\n",
    "  {\n",
    "      \"model\": \"gemini/gemini-pro\",             # May be overriden by the default gemini/gemini-pro\n",
    "      \"model_client_cls\": \"CustomModelClient\",\n",
    "  }\n",
    "]\n",
    "\n",
    "llm_config = {'config_list': config_list_custom}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=center> Autogen Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import AssistantAgent, UserProxyAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Agents\n",
    "\n",
    "Consturct a simple conversation between a User proxy and an Assistent agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 04-13 15:07:15] {418} INFO - Detected custom model client in config: CustomModelClient, model client can not be used until register_model_client is called.\n"
     ]
    }
   ],
   "source": [
    "assistant = AssistantAgent(\"assistant\", llm_config=llm_config)\n",
    "user_proxy = UserProxyAgent(\n",
    "    \"user_proxy\",\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False,  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the custom client class to the assistant agent\n",
    "On specifying the custom config list, autogen informs us that:  \n",
    "> [autogen.oai.client: 04-13 07:22:41] {419} INFO - Detected custom model client in config: CustomModelClient, model client can not be used until register_model_client is called.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant.register_model_client(model_client_cls=CustomModelClient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No default IOStream has been set, defaulting to IOConsole.\n",
      "No default IOStream has been set, defaulting to IOConsole.\n",
      "No default IOStream has been set, defaulting to IOConsole.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Write python code to print Hello World!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No default IOStream has been set, defaulting to IOConsole.\n",
      "No default IOStream has been set, defaulting to IOConsole.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "```python\n",
      "print(\"Hello World!\")\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No default IOStream has been set, defaulting to IOConsole.\n",
      "No default IOStream has been set, defaulting to IOConsole.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': 'Write python code to print Hello World!', 'role': 'assistant'}, {'content': '```python\\nprint(\"Hello World!\")\\n```', 'role': 'user'}], summary='```python\\nprint(\"Hello World!\")\\n```', cost=({'total_cost': 0}, {'total_cost': 0}), human_input=['exit'])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_proxy.initiate_chat(assistant, message=\"Write python code to print Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
